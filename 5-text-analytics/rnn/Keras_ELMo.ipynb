{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo embeddings example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JAXnSUgQ9tM"
   },
   "source": [
    "## **Install Tensorflow and TF Hub**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jdFDKgpQEZK",
    "outputId": "f81658cd-781e-4ad9-8082-c267665d1286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.18.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.46.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.15.0)\n",
      "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow\n",
    "!pip install -U tensorflow-hub\n",
    "!pip install -U numpy==1.18.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qADvp-w-RNEa",
    "outputId": "9562acdd-eecd-45f6-b83b-2551bd19a52c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 17 19:16:51 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8    25W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqK7E0aOQxw1",
    "outputId": "362c197f-43b7-426d-8553-970b5a761632"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f83c6d50081b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jRqrM-vVIaT"
   },
   "source": [
    "### **Tensorflow Hub: A MarketPlace for pretrained models**\n",
    "\n",
    "TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKJND9HAVPI6"
   },
   "source": [
    "### **ELMo - Deep contextualized word representations**\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/elmo-embedding.png\">\n",
    "\n",
    "***Inputs***\n",
    "\n",
    "The module defines two signatures: default, and tokens.\n",
    "\n",
    "* **default**: the module takes untokenized sentences as input. The input tensor is a string tensor with shape [batch_size]. The module tokenizes each string by splitting on spaces.\n",
    "\n",
    "* **tokens**: the module takes tokenized sentences as input. The input tensor is a string tensor with shape [batch_size, max_length] and an int32 tensor with shape [batch_size] corresponding to the sentence length. The length input is necessary to exclude padding in the case of sentences with varying length.\n",
    "\n",
    "***Outputs***\n",
    "\n",
    "The output dictionary contains:\n",
    "\n",
    "* **word_emb**: the character-based word representations with shape [batch_size, max_length, 512].\n",
    "* **lstm_outputs1**: the first LSTM hidden state with shape [batch_size, max_length, 1024].\n",
    "* **lstm_outputs2**: the second LSTM hidden state with shape [batch_size, max_length, 1024].\n",
    "* **elmo**: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape [batch_size, max_length, 1024].\n",
    "* **default**: a fixed mean-pooling of all contextualized word representations with shape [batch_size, 1024].\n",
    "\n",
    "\n",
    "We need to build a Keras layer as a wrapper of Tensorflow Hub ELMo module.\n",
    "\n",
    "See more in https://tfhub.dev/google/elmo/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WD5HZDDaZhfH",
    "outputId": "d7b549dc-064d-4883-af4e-c1fef9744734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"module_1_apply_default/aggregation/mul_3:0\", shape=(2, 6, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "\n",
    "embeddings = elmo(\n",
    "    [\"the cat is on the mat\", \"dogs are in the fog\"], signature=\"default\",\n",
    "    as_dict=True)[\"elmo\"]\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CBe8d48cRFLD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class ELMo(Layer):\n",
    "    def __init__(self, elmo_representation='elmo', trainable=True, **kwargs):\n",
    "        self.module_output = elmo_representation\n",
    "        self.trainable = trainable\n",
    "\n",
    "        self.elmo = None\n",
    "        super(ELMo, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # SetUp tensorflow Hub module\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/3',\n",
    "                               trainable=self.trainable, name=\"{}_module\".format(self.name))\n",
    "        \n",
    "        # Assign module's trainable weights to model\n",
    "        if self.trainable:\n",
    "          self._trainable_weights.extend(\n",
    "              tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "          )\n",
    "        \n",
    "        super(ELMo, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                           as_dict=True,\n",
    "                           signature='default',\n",
    "                           )[self.module_output]\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[0], 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0im5_LF6RTnL",
    "outputId": "2702b3fb-4443-4f24-e777-341c8ef0f7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "______________________________________________________________________________________________________________\n",
      "Layer (type)                        Output Shape            Param #      Connected to                         \n",
      "==============================================================================================================\n",
      "input_3 (InputLayer)                [(None, None)]          0                                                 \n",
      "______________________________________________________________________________________________________________\n",
      "input_4 (InputLayer)                [(None, 1)]             0                                                 \n",
      "______________________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)             (None, None, 50)        1000000      input_3[0][0]                        \n",
      "______________________________________________________________________________________________________________\n",
      "el_mo_1 (ELMo)                      (None, None, 1024)      4            input_4[0][0]                        \n",
      "______________________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)         (None, None, 1074)      0            embedding_1[0][0]                    \n",
      "                                                                         el_mo_1[0][0]                        \n",
      "______________________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)                 (None, None, 1074)      0            concatenate_1[0][0]                  \n",
      "______________________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)     (None, 600)             3300000      dropout_3[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)                 (None, 600)             0            bidirectional_1[0][0]                \n",
      "______________________________________________________________________________________________________________\n",
      "dense_2 (Dense)                     (None, 1000)            601000       dropout_4[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)                 (None, 1000)            0            dense_2[0][0]                        \n",
      "______________________________________________________________________________________________________________\n",
      "dense_3 (Dense)                     (None, 20)              20020        dropout_5[0][0]                      \n",
      "==============================================================================================================\n",
      "Total params: 4,921,024\n",
      "Trainable params: 4,921,024\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dropout, Bidirectional, LSTM\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, concatenate\n",
    "\n",
    "LSTM_SIZE = 300\n",
    "DENSE = 1000\n",
    "\n",
    "# Word ids of the sample\n",
    "inputs =  Input(shape=(None, ), dtype='int32')\n",
    "# And the strig as it is...\n",
    "elmo_inputs = Input(shape=(1,), dtype='string')\n",
    "\n",
    "# Define Embedding Layer (Actually these are random weights... You can use W2V!)\n",
    "embeddings = Embedding(20000, 50)(inputs)\n",
    "\n",
    "# ELMo embeddings as weighted sum across layers --> ['elmo'] module output\n",
    "elmo_embeddings = ELMo()(elmo_inputs)\n",
    "\n",
    "# Concat Word2Vec + ELMo embeddings in the last dimension (horizontally)\n",
    "concatenated_embeddings = concatenate([embeddings, elmo_embeddings], axis=-1)\n",
    "\n",
    "# Apply Dropout\n",
    "drop_emb = Dropout(0.33)(concatenated_embeddings)\n",
    "\n",
    "# Define an RNN (Biderectional) with LSTM cells\n",
    "bilstm = Bidirectional(LSTM(units=LSTM_SIZE, return_sequences=False, recurrent_dropout = 0.33))(drop_emb)\n",
    "\n",
    "# Apply Dropout to the bilstm representation\n",
    "drop_encodings = Dropout(0.33)(bilstm)\n",
    "\n",
    "# Pass through a Dense Layer\n",
    "hidden = Dense(units=DENSE, activation=\"relu\")(drop_encodings)\n",
    "\n",
    "# Apply Dropout to the output of the Dense Layer\n",
    "drop_out = Dropout(0.33)(hidden)\n",
    "\n",
    "# Last pass through a Dense Layer with softmax activation to produce a probability distribution\n",
    "out = Dense(units=20, activation=\"softmax\")(drop_out)\n",
    "\n",
    "# Wrap model --> Remember Functional API\n",
    "model = Model(inputs=[inputs, elmo_inputs], outputs=out)\n",
    "\n",
    "# Print topology\n",
    "model.summary(110)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ELMo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
